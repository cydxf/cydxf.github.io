嗯，last video I laid out the structure of a neural network, i'll give a quick weak at here just so that it's fresh in our minds and then I have to main goals for this video. The first is to introduce the idea of grade ient to sent, which under lies not only how narrow networks learn, but how a lot of the machine learning works as well, then after that we're going to dig in the little more to have this particular network performance and what those hidden layers of new ones and actually looking for as a reminder, our goal here is the classic example of hand written digit recognition, the hello world of neural networks, these digit sar and don a 28 by 28 pixel grid, each pixel with some Grace Gail value between zero and one goes r, what determine the activation of 784 neuron in the input layer of the network. And in the activation for each year on in the following layers is based on a waited sum of all the activation in the previous layer plus some special.
Number call to bias. Then you compos that some with some other function like the signal is education or array lu the way that I walk to last video in total, given the some what are but rich choice of two hidden layers here with 16 year on each. The network has about 13008 sin bias is that we can adjust and it's these values that determine what exactly the network you know actually does. Then what we mean when we say that this network classify is a given digit is that the Bright ists of those 10 ure ONS in the final layer correspond to that digit. And remember, the motivation that we had in mind here for the lay of structure was that maybe the second layer could pick up on the edges and the 3rd layer might pick up on patterns like loop and lines. And the last one could just piece together those patterns to recognize digit. So here we learn how the network learn, what we want is an algorithm where you can show this network a whole bunch of training day to which comes in the form of a bunch of different images.
Of hand written digit along with label for what they're supposed to be. And it will adjust those 13008 sin bias is so as to improve its performance on the training data. Hopefully this layer structure will mean that what it turns general eyes as to image is beyond that training data on the way we test that is that after u train the network you show it more label data that it's never seen before and you see how accurate lia classify new images. Fortunately for us and what makes this such a common example to start with is that the good people behind the database have put together a collection of tens of thousands of hand written digit images, each one label with the numbers that they're supposed to be. And it's profit of as it is to describe the machine is learning once you see how it works, it feels a lot less like some crazy I fi promise and a lot more like. I mean, basically it comes down to finding the minimum of a certain function.
Remember conceptual thinking of each nan as being connected to all of the nan ze in the previous layer and the weight sin the way did some defi ning its activation are kind of like the strength of those connection XR on the bias is some indication of whether that neuron tend to be active or in active on the output layer, it just looks like a mess. So what you do is you to find a cost function away of telling the computer, no bad computer that output should have activation witch r zero for most neuron, but one for this new on, what you gave me is other trash to say that a little more mathematics, what you do is add up the square of the differences between each of those trash output activation on the.
U that you want them to have. And this is what we call the cost of a single training example. Notice this some is small when the network confident classify the image correct li. But its large when the network seems like it doesn't really know what it's doing. So then what you do is considered the average cost over all of the tens of thousands of training examples at your dis pose. And that's a complicated things. Remember how the net work itself was basically a function one that takes in 784 numbers as in puts the pixel values and spit out 10 numbers as it's output, and innocence its parameter rise by all these weights and bias is while the cost function is a layer of complex ity on Top of that, it takes as it's in, put those 13000 or so weights and bias is on its bits out a single number describe ING.
How bad the's weights and bias is are on the way it's defined depends on the network behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crap job it's doing is it very helpful you want to tell it how to change the's rates and bias is so that it gets better to make it easier rather than struggling to imagine a function with 13000 and puts just imagine a simple function that has one number is an input on one number is and output. How do you find? And in put that mini mize is the value of this function. A more flex ible tactic is to start at any all in, put on, figure out which direction you should step to make that output lower.
Specifically, if you can figure out the slope of the function where you are then shift to the left, if that slope is positive and shift the input to the right, if that slope is negative, if you do this repeated Lee at each point, checking the noose lope and taking the appropriate step, you are going to approach some local minimum of the function. On the image you might have in mind here is a ball rolling down a Hill. A notice, even for this really simple, if i'd single input function, there are many possible val is that you might land in depending on which brand of input you start at, and there is no guarantee that the local minimum you land on is going to be the small is possible value of the cost function that's going to carry over to our own network case as well. And I also want you to notice how if you make your step size is proportion to the slope, then when the slope is flat on ING out to wards the minimum, your steps gets smaller and smaller, and that kind of helps you from over shooting bum ping up the complex ity of it, imagine instead of function with to input.
And one output, you might think of the input space as the xy plane on the cost function as being graph as a service above it. Now, instead of asking about the slope of the function, you have to ask, which direction should you step in this in put space, so as to de crease, the output of the function most quickly. In other words, what's the down hail direction? And again, it's helpful to think of the ball rolling down that Hill. Those of you for me Leah multi variable cal cula will know that the grad ient of a function gives you the direction of steep sent, basically which direction xt a increase the function most quickly. Naturally enough taking the negative of that grade ient gives you the direction to step that de crease is the function most quickly. And even more than that, the length of this grad ient vector is actually an indication for just how steep that's deep slope is. Now if your UN familiar with multi variable cal cula Adam on the.
Topic. You'll be okay if that's all you know and you're not Rock solid on the details, it's the same basic idea for a function that has 13000 and puts in stead of to input. Imagine organ izing all through teenth aos and weeks and bias is of our network into a giant column vector. The negative grad ient of the cost function is just a vector its some direction inside this in saint Lee, huge input space that tells you which nudge is to all of those numbers, is going to cause the most rapid de crease to the cost function. And of course, with our special e design cost function, changing the weight and buy it.
Is to de crease, it means making the output of the network on each piece of training day to look less like a random a of 10 values and more like an actual decision that we want it to make. It's important to remember this cost function involved in average over all of the training data. So if you mini mize, it it means it's a better performance on all of those samples. The algorithm for Computing this great efficient li, which is effectively the heart of how a neural network learning is called back profession, and it's what I'm gonna be talking about next video. There I really want to take the time to walk through what exactly happens to each wait and each bias for a given piece of training day to trying to give and in tu itive feel for what's happening beyond the pile of relevant right here. Right now, the main thing I want you to know independent of implementation details is that what we mean when we talk about a network learning is that it's just minim izing a cost function. And notice one.
Consequence of that is that it's important for this cost function to have a nice smooth out put so that we can find the local minimum by taking little steps down Hill. This is why by the way, artificial neural XR have continu ously rain jing activation rather than simply being active or in active in a minor re the way that biological neural ZARA. This process of repeated Lena jing and input of a function buy some multiple of the negative grad ient is called grad ient, dis ent its a way to convert to ward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a function with two in puts, of course, because not is in a 13000 dimensional into its base, are a little hard to wrap your mind around, but there is actually a nice nan special way to think about this. Each component of the negative grad ient tells us to things the sign, of course, tells us whether the correspond ING component of the input vector should be noted up or down, but important the relative magnet of all these components.
Kind of tells you which changes matter more wr. Is he in our network and a judgment to one of the weight might have a much greater impact on the cost function than the adjust ment to some other way. Some of the connection is just matter more for our training data. So a way that you can think about this grad ient vector of our mind, walking lim as of cost function, is that it includes the relative importance of each way to nd's that is which of these changes is going to carry the most Bang for your buck comes out as 31. Then on the one hand, you can inter prit that as saying that when you're standing at that in put moving along this direction increases the function most quickly, that when u graph the function above the plane of input points that very.
To is what giving you the straight up Hill direction. But another way to read that is to say that changes to this first variable have three times the importance to that, at least in the neighborhood of the relevant input. Nothing the x value Carrie's a lot more bang for your buck. All right, let's zoom out and some up where we are so far the net work itself is this function with 784 in puts on 10 out puts defined in terms of all of these waited sims. The cost function is a layer of complex ity on Top of that it takes the 13000 and bias is as input and spit out a single measure of lao zi OS based on the training examples. On the grad ient of the cost function is one more layer of complex ity. Still, it tells us what nudge is to all of these weights and bias is because the fast is change to the value of the cost function, which you might interpret as saying, which changes to which way to matter the most.
So when you initial ize the network with random weights and bias is and had just than many times based on this grad ient to send process, how well does it actually perform on image is that it's never seen before. While the one that I have described here with the two hidden layers of 16 yuan zi chosen mostly for STH etic reasons. And honestly, if you look at some of the examples that it is up on you kind of field compared to cut it a little black, but give and how daun ting the initial task is, I just think there is something incredible about any network doing this well and images that it's never seen before, given that we never specifically told that what.
The patterns to look for originally the way that I motive ated the structure was by describe ING a hope that we might have that the second layer might pick up on little edges, that the 3rd layer wood piece together those edges to recognize lips and longer lines. And that those might be peace together to recognize digit. So is this what our network is actually doing? Well, for this one, at least not at all, remember how video we looked at, how the weight of the connection from all of the runs in the first layer to a given year on in the second layer. They look well almost random just but some very lose patterns in the middle there. It would seem that in the UN family, large, dirty thousand dimensional space of possible way to NBA.
Our network found itself a happy little local minimum that despite successfully classify ING, most images doesn't exactly pick up on the patterns that we might have hope for. And to really drive this point home, watch what happens when you in put a random image. If the system was a smart, you might expect it to either feel UN certain maybe not really act of a ting, any of those 10 output neuron or active ating the mall even Lee. But instead it confident gives you some nonsense answer as if it feels as sure that this random nor actual image of a 5 is a 5. A lot of this is because it's such a title IQ and strain training set up. I mean, put yourself in the network she was here from its point of view, the entire universe consists of nothing, but clearly to find on moving digit center dinner, tiny grid. And its cost function just never give it any in cent of to be anything but utterly.
Fit in the decisions. Well, this is not meant to be our end go, but instead of starting point, frankly, this is old technology and the kind research in the eighties and 90 and you do need to understand it before you can understand more detailed modern dance. And it clearly is capable of solve ING some interesting problems. But the more you dig into what those hidden layers are really doing, the less intelligent it seems, shift ING the focus for a moment from how networks learn to how you learn that will only happen if you in a job. One pretty simple thing that I want to do is just cause right now and think people for a moment about what changes you might make to this system on how it perceived image is if you want it to better pick up on things like.
Edges and patterns. But better than that to actually engage with the material, I highly recommend the book by Michael Nelson on deep learning on neural networks in it, you can find the code and the data to download and play with for this exact on the book will walk you through step by step what that code is doing. What's awesome is that this book is Free in public. Lia valent efforts have also link to couple of the resources that I like a lot in the description, including the phenomenon beautiful blog post by Chris o LE and the articles in the still to close things off here for the last few minutes, I want to jump back into a pit of the interview that I had with Lisa li, you might remember her from the last video she did her PHD work in deep learning and in this little snip it, she talks about to recent papers that really dig into how some of the more modern image recognition networks are actually learning just to set up where we were in the conversation, the first paper took one of these.
Deep neural networks that's really good at image recognition and instead of training it on a properly label, dataset it, shuffle dall of the levels of round before training. Obviously, the testing accuracy here was going to be no better than random, since everything is just random label. But it was still able to achieve the same training accuracy as you would on a proper label dataset. Basically, the millions of wait for this particular network, we are enough for it to just memories the random day to which kind of is the question for weather minim izing, this cost function actually correspond to any sort of structure in the image, or is it just u know memories of the entire day to set of what the correct classification is and soak up a love you no half a year later on I am all this year there is not exactly the bottle paper paper that are just some aspects of like, hey, actually the networks are doing something a little bit. That accuracy curve if you were just training on a random dataset that curve.
Went down very very slowly and almost kind of Olivia fashion, so you are really struggling to find that local minimum of possible you know that the right way that would get you that accuracy, whereas if you actually training on a structure dataset one that has the right label, you know you around a little bit in the beginning, but then you could have dropped a very fast to get to that accuracy level. And so in some sense, it was easier to find that local maximum. And so it was so interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simply fish and about the network layers. But one of the result was saying how if you look at the optim isation landscape, the local minimum that these networks tend to learn are actually a equal qualities and some sense, if your data set of structure, you should be able to find that much more easily. My thanks as always to.
Those of you supporting on a tree and i've said before, just what a game up a tree on is, but these videos really would not be possible without you. I also want to give a special thanks to the vc firm am ple. If I partners in their support of these initial videos in the series, they focus on very early stage machine learning in a A.I. companies and I feel pretty confident in the probability that some of you watching this and even more likely some of the people that you know are right now in the early stage as of getting on the apple of my folks would love to hear from any such found as on the eve and set up an Email address just for this video that you can reach out to them through three blue one brown at if I partners dot com.
